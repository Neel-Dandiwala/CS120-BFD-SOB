{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XU3_aRitCbl",
    "outputId": "281ebaae-d5eb-412b-df49-f03d792de796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
      "     -------------------------------------- 70.1/70.1 KB 478.8 kB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "     -------------------------------------- 77.1/77.1 KB 857.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from openai) (2.28.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-win_amd64.whl (319 kB)\n",
      "     -------------------------------------- 319.8/319.8 KB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.20->openai) (2022.9.24)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp310-cp310-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.1/56.1 KB 3.1 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-win_amd64.whl (33 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\neeldandiwala\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->openai) (0.4.5)\n",
      "Installing collected packages: tqdm, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 tqdm-4.65.0 yarl-1.8.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\NeelDandiwala\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0j_g-zLr-0c",
    "outputId": "72ad9fbf-8d57-4de9-a8fb-33d86203ff78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.21.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.21.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "faMFqwiBwrog"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "# with open('/content/openai_key.json', 'r') as file_to_read:\n",
    "#     json_data = json.load(file_to_read)\n",
    "#     OPENAI_API_KEY = json_data[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SVaUwapRw5GH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key =  OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_N08al2w9X_"
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open('/content/Text Summarization with Pretrained Encoders.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHyBoPabxQbn",
    "outputId": "1236f70b-7f6e-40a9-b163-a327266509bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paper examines the use of BERT in text summarization, proposing a general framework for both extractive and abstractive models. A novel document-level encoder is proposed which combines a pretrained BERT encoder with a randomly initialized Transformer decoder. Experiments on three datasets show that this model achieves state-of-the-art results for both extractive and abstractive summarization.  We propose a two-stage approach to\n",
      "summarization with pretrained language models. We\n",
      "experimentally show that the proposed models achieve\n",
      "state-of-the-art results on three single-document news\n",
      "summarization datasets. Our contributions are three-fold:\n",
      "we highlight the importance of document encoding for\n",
      "the summarization task, showcase ways to effectively\n",
      "employ pretrained language models in summarization\n",
      "under both extractive and abstractive settings, and\n",
      "provide models  BERT has been used to fine-tune various NLP tasks and is also applicable to summarization. It can be extended by inserting multiple [CLS] symbols to learn sentence representations, using interval segmentation embeddings to distinguish multiple sentences, and using contextual embeddings for each token. SUMMARUNNER, REFRESH, LATENT, SUMO, NEUSUM are all neural approaches that have been applied to extractive summarization. For abstractive summarization, Rush  We present BERTSUM, a BERT-based architecture for summarization. It has an encoder, which is BERT, and a decoder which is a transformer with multiple layers. We use a sigmoid classifier to assign labels to each sentence in the document in order to create an extractive summary. For abstractive summarization, we use an encoder-decoder framework, with BERTSUM as the encoder and a transformer initialized randomly as the decoder  We evaluated our model on three benchmark summarization datasets: CNN/DailyMail, New York Times Annotated Corpus (NYT) and XSum. We used the standard splits of Hermann et al. (2015) for training, validation, and testing the CNN/DailyMail dataset. We split NYT into 100,834/9,706 training/test examples based on the date of publication and 4,000 examples from the training as validation set. The XSum dataset was split into \n",
      "\n",
      "We evaluated summarization quality automatically using ROUGE and report unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a means of assessing informativeness and the longest common subsequence (ROUGE-L) as a means of assessing fluency. Results for various extractive and abstractive models are presented, along with our BERT-based models. Our BERT-based extractive model achieves the best performance, while our B  BERT-based models outperform previously proposed extractive and abstractive systems on summarization tasks, with BERTSUMEXT achieving the best performance. Abstractive BERT models approach ORACLE upper bound performance on some datasets.  We compare our BERT summarizers to previous models, and find that they perform much better. We also examine the effects of learning rates, the positions of extracted sentences, and novel n-grams in summaries. Finally, we conduct human evaluation with a question-answering paradigm and Best-Worst Scaling.  We presented a novel framework using BERT for both abstractive and extractive summarization. Our model achieved state-of-the-art results on three datasets and was preferred by participants in human evaluations. Future work includes taking advantage of BERT for language generation.  Layer normalization is a method for normalizing the activations of a neural network. It was introduced in 2016 by Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. MMR and diversity-based reranking are used for reordering documents and producing summaries. Deep communicating agents for abstractive summarization were introduced in 2018. Discourse constraints for document compression were introduced in 2010. BERT is a pre-trained deep bidirectional transformer for language understanding which was introduced in 2019. \n",
      " \n",
      " Deep learning-based methods have been used to develop abstractive and extractive summarization models. These models use techniques such as sequence-to-sequence RNNs, topic-aware CNNs, reinforcement learning, generative pre-training, pointer-generator networks, neural attention models, and hierarchical bidirectional transformers. A variety of datasets, such as the New York Times Annotated Corpus, have been used for training and evaluation.\n"
     ]
    }
   ],
   "source": [
    "# summary_list =[]\n",
    "# for page in doc:\n",
    "#   text = page.get_text(\"text\")\n",
    "#   #print(text)\n",
    "  prompt= text + \"\\n Tl;dr:\"\n",
    "  response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=prompt,\n",
    "  temperature=0.7,\n",
    "  max_tokens=100,\n",
    "  top_p=0.9,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=1\n",
    "  )\n",
    "  summary_list.append(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "summary_text=' '.join(summary_list)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
